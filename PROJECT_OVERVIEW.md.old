# AGR Local LLM Project Overview

## Purpose

This repository provides **local, self-hosted AI model services** for Alliance of Genome Resources (AGR) applications, eliminating dependency on external API providers and reducing costs.

## Current Capabilities

### 1. Embedding API (Production)
**Location**: Root directory
**Status**: âœ… Production-ready
**Purpose**: Generate high-quality text embeddings for semantic search and similarity matching

**Model**: Qwen3-Embedding-8B (4096-dimensional embeddings)
**Performance**: Optimized for 96-core CPU systems
**API**: FastAPI server with OpenAPI docs

See [README.md](README.md) for full embedding API documentation.

---

### 2. Granite 4.0 LLM API (New - In Development)
**Location**: `granite-4.0/` directory
**Status**: ğŸš§ Ready for deployment and testing
**Purpose**: Self-hosted LLM for CrewAI agents in AI Curation application

**Model**: IBM Granite 4.0 H-Micro (3B parameters)
**Architecture**: Hybrid Mamba for fast CPU inference
**API**: OpenAI-compatible endpoint via llama.cpp
**Integration**: Drop-in replacement for OpenAI API in CrewAI

#### Key Files
```
granite-4.0/
â”œâ”€â”€ README.md                      # Model overview and capabilities
â”œâ”€â”€ CREWAI_INTEGRATION.md          # How to integrate with AI Curation
â”œâ”€â”€ QUICKSTART.md                  # Step-by-step deployment guide
â”œâ”€â”€ Dockerfile                     # Production container image
â”œâ”€â”€ docker-compose.yml             # Easy deployment config
â”œâ”€â”€ deploy.sh                      # Deployment automation script
â”œâ”€â”€ .env.example                   # Configuration template
â””â”€â”€ test_crewai_integration.py     # Integration test script
```

#### Quick Start
```bash
cd granite-4.0/
./deploy.sh build   # Build container (~10-15 min first time)
./deploy.sh start   # Start API server
./deploy.sh test    # Run tests
```

See [granite-4.0/QUICKSTART.md](granite-4.0/QUICKSTART.md) for detailed instructions.

---

## System Specifications

**Server**: FlySQL26
**CPU**: Dual Intel Xeon Gold 6240R (96 threads total)
**Architecture**: x86_64 with AVX-512 support
**Optimization**: Both services optimized for CPU-only inference

See [SYSTEM_SPECS.md](SYSTEM_SPECS.md) for complete hardware details.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FlySQL26 Server                         â”‚
â”‚                   (96 CPU Threads, AVX-512)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Embedding API        â”‚  â”‚   Granite 4.0 LLM API   â”‚   â”‚
â”‚  â”‚   (Port 9000)          â”‚  â”‚   (Port 8080)           â”‚   â”‚
â”‚  â”‚                        â”‚  â”‚                         â”‚   â”‚
â”‚  â”‚  Qwen3-Embedding-8B    â”‚  â”‚  Granite 4.0 H-Micro    â”‚   â”‚
â”‚  â”‚  FastAPI Server        â”‚  â”‚  llama.cpp Server       â”‚   â”‚
â”‚  â”‚  4096-dim Embeddings   â”‚  â”‚  OpenAI-compatible API  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                            â”‚                     â”‚
â”‚           â”‚                            â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                            â”‚
            â–¼                            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   RAG/Search  â”‚          â”‚  AI Curation     â”‚
    â”‚   Pipeline    â”‚          â”‚  (CrewAI Agents) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Cost Savings

### Embedding API
- **OpenAI Equivalent**: text-embedding-3-large ($0.13 per 1M tokens)
- **Our Cost**: Infrastructure only (~$0)
- **Quality**: Comparable or better (MTEB #1 multilingual)

### Granite 4.0 LLM
- **OpenAI Equivalent**: gpt-4o-mini ($0.15-$0.60 per 1M tokens)
- **Our Cost**: Infrastructure only (~$0)
- **Expected Savings**: 100% of LLM API costs for AI Curation

---

## Performance Expectations

### Embedding API (Current)
- **First request**: 10-30 seconds (model compilation)
- **Subsequent requests**: 1-5 seconds for small batches
- **Batch optimization**: Best with 8-32 texts

### Granite 4.0 LLM (Estimated)
- **First token latency**: ~100-200ms
- **Throughput**: 50-100 tokens/second
- **Context window**: Up to 128K tokens
- **Concurrent sessions**: 10+ supported

---

## Integration Points

### 1. Embedding API
**Current Users**:
- Any application needing semantic embeddings
- Document similarity search
- Retrieval-Augmented Generation (RAG)

**API Format**: Custom FastAPI endpoints

### 2. Granite 4.0 LLM
**Target User**: AI Curation Prototype (CrewAI-based)
**Integration Method**:
- Drop-in replacement via `base_url` parameter
- OpenAI-compatible API format
- No code changes required in agents

**Example**:
```python
from crewai import LLM

llm = LLM(
    model="granite-4.0-h-micro",
    base_url="http://flysql26:8080/v1",
    api_key="dummy-key"
)
```

---

## Deployment Status

| Service | Status | Port | Health Check |
|---------|--------|------|--------------|
| Embedding API | âœ… Production | 9000 | `GET /health` |
| Granite LLM | ğŸš§ Ready | 8080 | `GET /health` |

---

## Next Steps

### Immediate (Granite 4.0)
1. âœ… Complete documentation and deployment scripts
2. â¬œ Deploy to FlySQL26 and run initial tests
3. â¬œ Integrate with AI Curation development environment
4. â¬œ Benchmark performance vs OpenAI
5. â¬œ Deploy to production with fallback mechanism

### Future Enhancements
1. **Monitoring**: Add Prometheus metrics for both services
2. **Load Balancing**: Scale horizontally if needed
3. **Model Updates**: Test newer Granite versions as released
4. **Additional Models**: Evaluate other open-source LLMs

---

## Repository Structure

```
agr_local_llm/
â”œâ”€â”€ PROJECT_OVERVIEW.md           # This file
â”œâ”€â”€ SYSTEM_SPECS.md               # Hardware specifications
â”œâ”€â”€ README.md                     # Embedding API documentation
â”œâ”€â”€ server.py                     # Embedding API server
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ Dockerfile                    # Embedding API container
â”œâ”€â”€ deploy.sh                     # Embedding API deployment
â”‚
â””â”€â”€ granite-4.0/                  # Granite LLM service
    â”œâ”€â”€ README.md                 # Model overview
    â”œâ”€â”€ QUICKSTART.md             # Deployment guide
    â”œâ”€â”€ CREWAI_INTEGRATION.md     # Integration details
    â”œâ”€â”€ Dockerfile                # LLM API container
    â”œâ”€â”€ docker-compose.yml        # Deployment config
    â”œâ”€â”€ deploy.sh                 # Deployment automation
    â””â”€â”€ test_crewai_integration.py # Integration tests
```

---

## Support & Maintenance

**Owner**: Chris Tabone
**Purpose**: Production services for AGR applications
**Server**: FlySQL26 (flysql26.alliancegenome.org)

For issues or questions, consult the specific README files in each directory.

---

*Last Updated: October 3, 2025*
